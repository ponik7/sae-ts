{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e312ae-85e3-4aa3-8226-53351ab783a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fe44373-c7de-4b09-9298-f2b914686e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49a0f2b6-ed6d-457f-ab50-227274464084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /home/podsevatkin.n/.local/lib/python3.10/site-packages (1.3.5)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: tqdm>4 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/podsevatkin.n/.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "903a2e8a-d6ef-4bf0-9a8b-ad9316825a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64cbb2074d447a08208ef8af16ff16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cbc94fc-af98-4f84-9357-6d10c44b2486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b063dc4c436846a7a53d9f21cfd43ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2773fb76c11642af82d05668492e0d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e80e86cf784706bde59a43d2edda92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3f6acd8e5042a09d7f50f53c7583b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6f174390094c4fb76821d00b0d6cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb7272dfbf2496d9b85997f0602ede8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d95bcecf33438e84f71c168fe44fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fa69d117b546c8a04700b349bd2b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99aa86f5b21c4cf19bafbf2ee92e1e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242d46b281f94929acb8916ce93a7700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08980c68c1684073913b9d18458f64d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False) # avoid blowing up mem\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    device_map='auto',\n",
    ")\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35fede1a-6b7b-4001-9e1b-cdf8d4fce03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,   6571,    708,    692, 235336]], device='cuda:0')\n",
      "<bos>Who are you?\n",
      "\n",
      "I am Gemma, an AI assistant created by the Gemma team. I am a large language model, which means I'm trained on a massive amount of text data. This allows me to understand and generate human-like text in response to a\n"
     ]
    }
   ],
   "source": [
    "# The input text\n",
    "prompt = \"Who are you?\"\n",
    "\n",
    "# Use the tokenizer to convert it to tokens. Note that this implicitly adds a special \"Beginning of Sequence\" or <bos> token to the start\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n",
    "print(inputs)\n",
    "\n",
    "# Pass it in to the model and generate text\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eddab262-f305-4a54-a7c3-00ac3242d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_params = hf_hub_download(\n",
    "    repo_id=\"google/gemma-scope-2b-pt-res\",\n",
    "    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n",
    "    force_download=False,\n",
    ")\n",
    "params = np.load(path_to_params)\n",
    "pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb6e533-5dfd-44e0-9d2f-fc0198427806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_dec': torch.Size([16384, 2304]),\n",
       " 'W_enc': torch.Size([2304, 16384]),\n",
       " 'b_dec': torch.Size([2304]),\n",
       " 'b_enc': torch.Size([16384]),\n",
       " 'threshold': torch.Size([16384])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v.shape for k, v in pt_params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389d9937-d5a2-40f4-a6c3-3792289a2a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2101, 1.1695, 0.9836,  ..., 1.0630, 0.9997, 1.1070], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_params[\"W_enc\"].norm(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f65afc5-9533-4859-b0d5-c8573dc43235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class JumpReLUSAE(nn.Module):\n",
    "  def __init__(self, d_model, d_sae):\n",
    "    # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
    "    # If you want to train your own SAEs then we recommend using blah\n",
    "    super().__init__()\n",
    "    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
    "    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
    "    self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
    "    self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "  def encode(self, input_acts):\n",
    "    pre_acts = input_acts @ self.W_enc + self.b_enc\n",
    "    mask = (pre_acts > self.threshold)\n",
    "    acts = mask * torch.nn.functional.relu(pre_acts)\n",
    "    return acts\n",
    "\n",
    "  def decode(self, acts):\n",
    "    return acts @ self.W_dec + self.b_dec\n",
    "\n",
    "  def forward(self, acts):\n",
    "    acts = self.encode(acts)\n",
    "    recon = self.decode(acts)\n",
    "    return recon\n",
    "\n",
    "\n",
    "sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
    "sae.load_state_dict(pt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffe8be58-e9a9-492d-af26-a0486058c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "  target_act = None\n",
    "  def gather_target_act_hook(mod, inputs, outputs):\n",
    "    nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
    "    target_act = outputs[0]\n",
    "    return outputs\n",
    "  handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
    "  _ = model.forward(inputs)\n",
    "  handle.remove()\n",
    "  return target_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8364d200-8afc-4b25-ac8c-03dbd3aaf4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_act = gather_residual_activations(model, 20, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1844dfdf-20cb-4520-ac2b-a0b7cf664947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2304])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_act.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e709ea9d-cb0a-47e5-a779-4f289fa7caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JumpReLUSAE()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sae.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f74730e7-37a1-4f9d-8445-1886cc5dd051",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "recon = sae.decode(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ed4ebcc-0aa8-47a8-ac7f-dcad89689c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_vector = sae.W_dec[12332]\n",
    "decoded_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4fab8ec-dcda-47db-8fab-77d8210adaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_injection_hook(steered_residual):\n",
    "#     def hook_fn(module, inputs, outputs):\n",
    "#         # print(outputs)\n",
    "#         assert steered_residual.shape == outputs[0].shape, f\"Shape mismatch blyat. Steered {steered_residual.shape} and {outputs[0].shape}\"\n",
    "#         print(\"all good\")\n",
    "#         return (steered_residual, outputs[1])\n",
    "#     return hook_fn\n",
    "\n",
    "# def make_injection_hook(steered_residual):\n",
    "#     def hook_fn(module, inputs, outputs):\n",
    "#         h, cache = outputs\n",
    "#         L = h.shape[1]   # 5 on the first pass, 1 thereafter\n",
    "#         injection = steered_residual[:, -L:, :]\n",
    "#         assert injection.shape == h.shape\n",
    "#         return (injection, cache)\n",
    "#     return hook_fn\n",
    "\n",
    "\n",
    "\n",
    "def make_injection_hook(decoded_vec, alpha=0.1):\n",
    "    def hook_fn(module, inputs, outputs):\n",
    "        h, cache = outputs\n",
    "        L = h.shape[1]                  # 5 on context, 1 on new tokens\n",
    "        # slice_vec = steered_residual[:, -L:, :]\n",
    "        # optionally scale down the effect on new tokens\n",
    "        injection = h + alpha * decoded_vec\n",
    "        return (injection, cache)\n",
    "    return hook_fn\n",
    "\n",
    "hook = model.model.layers[20].register_forward_hook(\n",
    "    make_injection_hook(decoded_vector, alpha=122)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b43e43-d964-4d51-a23f-2eb40909eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36af4c3c-502d-4caf-805a-5436cc828727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Who are you?\n",
      "\n",
      "Je suis une intelligence artificielle, un modÃ¨le de langage dÃ©veloppÃ© par Google. \n",
      "\n",
      "Comment puis-je vous aider ? \n",
      "\n",
      "Je peux vous aider Ã  :\n",
      "\n",
      "* **RÃ©pondre Ã  vos questions** sur une variÃ©tÃ© de sujets.\n",
      "* **\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f5d17-2b34-4169-bd46-7f8ef4b2e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f69bf4-5b42-47de-91b6-fce245731627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6475d7a-850b-452e-999e-f5088f655326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_hw",
   "language": "python",
   "name": "ai_hw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
